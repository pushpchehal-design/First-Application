{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805f0680",
   "metadata": {},
   "source": [
    "# Gemini 2.0 Flash basics (text and text+image)\n",
    "\n",
    "This notebook shows how to call Google's Gemini 2.0 Flash model for:\n",
    "- Text generation\n",
    "- Multi-turn chat\n",
    "- Text + image prompts\n",
    "\n",
    "It also explains key generation parameters and how to obtain an API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7acbdaf",
   "metadata": {},
   "source": [
    "### get your api key from google ai studio\n",
    "\n",
    "1. Visit **Google AI Studio** at `https://ai.google.dev` and sign in with your Google account.  \n",
    "2. Go to **Get API key** (or **API keys**) in the left navigation.  \n",
    "3. Click **Create API key**, choose the project if prompted, and copy the key.  \n",
    "4. (Optional but recommended) Restrict the key to your environment and rotate it periodically.  \n",
    "5. Store it securely, e.g., in an environment variable `GEMINI_API_KEY`.\n",
    "\n",
    "> Note: The UI labels may change slightly over time. If something looks different, search for \"API keys\" within AI Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9012b0c9",
   "metadata": {},
   "source": [
    "### install dependencies\n",
    "\n",
    "```bash\n",
    "pip install --upgrade google-generativeai pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed69b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client configured. Model: gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavishya/VSC Projects/Sessions/HoE/First GenAI App/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# configure the client\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Prefer environment variable; fall back to an input prompt (not echoed)\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create a model handle. If you get a \"model not found\" error,\n",
    "# upgrade the SDK and confirm the model name you have access to.\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "model = genai.GenerativeModel(\n",
    "    model_name,\n",
    "    system_instruction=\"You are a concise, helpful assistant.\"\n",
    ")\n",
    "print(\"Client configured. Model:\", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c51b9",
   "metadata": {},
   "source": [
    "### generation parameters you will use\n",
    "\n",
    "- **temperature**: randomness; higher = more creative, lower = more deterministic.  \n",
    "- **top_p**: nucleus sampling; consider tokens from the smallest set whose cumulative probability â‰¥ *p*.  \n",
    "- **top_k**: sample only from the top-*k* most probable tokens at each step.  \n",
    "- **max_output_tokens**: hard limit on the length of the model's output.  \n",
    "- **response_mime_type**: preferred format, e.g., `\"text/plain\"` or `\"application/json\"`.  \n",
    "- **safety_settings**: optional policy filters for categories and thresholds.\n",
    "\n",
    "Use these with `generation_config={...}` on each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee540f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   **Improved Clarity:** Concise documentation reduces ambiguity, making it easier for users to understand complex information.\n",
      "*   **Increased Efficiency:** Shorter documentation saves time for both writers and readers, accelerating the learning and implementation process.\n",
      "*   **Reduced Errors:** Clear and direct instructions minimize the chance of misinterpretation and subsequent errors.\n",
      "*   **Enhanced Maintainability:** Concise documents are easier to update and maintain, ensuring accuracy and relevance over time.\n",
      "*   **Better User Experience:** Respect\n"
     ]
    }
   ],
   "source": [
    "# simple text generation with parameters\n",
    "generation_config = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 100,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "prompt = \"In 5 bullet points, explain the benefits of writing concise technical documentation.\"\n",
    "resp = model.generate_content(prompt, generation_config=generation_config)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6d2b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Okay, here are two fun facts about honeybees:\n",
      "\n",
      "1.  **Honeybees communicate through dance:** They perform a \"waggle dance\" to tell other bees the direction and distance of food sources.\n",
      "2.  **Honeybees can recognize human faces:** Studies have shown they can distinguish between different human faces, which is impressive for such a small brain!\n",
      " \n",
      "\n",
      "Assistant: Honeybees communicate through a waggle dance and can recognize human faces.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multi-turn chat\n",
    "chat = model.start_chat(history=[\n",
    "    {\"role\": \"user\", \"parts\": \"Hello!\"},\n",
    "    {\"role\": \"model\", \"parts\": \"Hi there! How can I help today?\"},\n",
    "])\n",
    "\n",
    "r1 = chat.send_message(\"Give me two fun facts about honeybees.\")\n",
    "print(\"Assistant:\", r1.text, \"\\n\")\n",
    "\n",
    "r2 = chat.send_message(\"Now summarize that in one sentence.\")\n",
    "print(\"Assistant:\", r2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33c084",
   "metadata": {},
   "source": [
    "### text + image prompt\n",
    "\n",
    "You can pass images along with text. Provide each image as a dict with `mime_type` and raw `data` bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba1458e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the image, I see the Mona Lisa, a famous portrait painted by Leonardo da Vinci.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text + image example\n",
    "from pathlib import Path\n",
    "\n",
    "# Replace this path with a local image path on your machine.\n",
    "image_path = Path(\"images.jpeg\")\n",
    "\n",
    "# Load image bytes if available\n",
    "parts = []\n",
    "if image_path.exists():\n",
    "    img_bytes = image_path.read_bytes()\n",
    "    parts.append({\n",
    "        \"mime_type\": \"image/jpeg\" if image_path.suffix.lower() in [\".jpg\", \".jpeg\"] else \"image/png\",\n",
    "        \"data\": img_bytes,\n",
    "    })\n",
    "else:\n",
    "    print(\"Note: sample image not found. The request will be text-only.\")\n",
    "\n",
    "parts.append(\"Who do you see in this image?\")\n",
    "resp = model.generate_content(parts, generation_config={\n",
    "    \"temperature\": 0.4,\n",
    "    \"max_output_tokens\": 200,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "})\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24402da",
   "metadata": {},
   "source": [
    "### troubleshooting tips\n",
    "\n",
    "- If you see `PermissionDenied` or `NotFound`, verify you have access to the model name and that your SDK is up to date.  \n",
    "- Ensure your `GEMINI_API_KEY` is set and valid.  \n",
    "- For large images, downscale before sending to reduce latency.  \n",
    "- Handle errors with try/except and show helpful messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
